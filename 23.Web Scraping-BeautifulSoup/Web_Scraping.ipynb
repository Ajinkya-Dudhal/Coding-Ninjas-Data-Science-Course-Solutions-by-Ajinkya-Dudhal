{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b1a8854",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7d490ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = \"<html>\\\n",
    "    <head>\\\n",
    "        <title> Testing Web Page</title>\\\n",
    "    </head>\\\n",
    "    <body>\\\n",
    "        <h1> Web Scraping</h1>\\\n",
    "        <p class='abc' id='first_para'>\\\n",
    "            Let's start learning\\\n",
    "            <b>\\\n",
    "                Web Scraping\\\n",
    "            </b>\\\n",
    "        </p>\\\n",
    "        <p id='def'>\\\n",
    "            You can read more about BeautifulSoup from <a href='https://www.crummy.com/software/BeautifulSoup/bs4/doc/'> here </a>\\\n",
    "        </p>\\\n",
    "        <p class='abc'>\\\n",
    "            <a href='https://codingninjas.in/'> Coding Ninjas </a>\\\n",
    "        </p>\\\n",
    "    </body>\\\n",
    "</html>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf0dc674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html> <head> <title> Testing Web Page</title> </head> <body> <h1> Web Scraping</h1> <p class=\"abc\" id=\"first_para\">            Let's start learning            <b>                Web Scraping            </b> </p> <p id=\"def\">            You can read more about BeautifulSoup from <a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\"> here </a> </p> <p class=\"abc\"> <a href=\"https://codingninjas.in/\"> Coding Ninjas </a> </p> </body></html>\n",
      "<class 'bs4.BeautifulSoup'>\n"
     ]
    }
   ],
   "source": [
    "data = BeautifulSoup(html,'html.parser')\n",
    "print(data)\n",
    "print(type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83b6d93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   Testing Web Page\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <h1>\n",
      "   Web Scraping\n",
      "  </h1>\n",
      "  <p class=\"abc\" id=\"first_para\">\n",
      "   Let's start learning\n",
      "   <b>\n",
      "    Web Scraping\n",
      "   </b>\n",
      "  </p>\n",
      "  <p id=\"def\">\n",
      "   You can read more about BeautifulSoup from\n",
      "   <a '=\"\" href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\">\n",
      "    here\n",
      "   </a>\n",
      "  </p>\n",
      "  <p class=\"abc\">\n",
      "   <a '=\"\" href=\"https://codingninjas.in/\">\n",
      "    Coding Ninjas\n",
      "   </a>\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f91c6f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title> Testing Web Page</title>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5ff9e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<head> <title> Testing Web Page</title> </head>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5862944b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h1> Web Scraping</h1>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5069f574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p class=\"abc\" id=\"first_para\">            Let's start learning            <b>                Web Scraping            </b> </p>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2863d99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title\n",
      " Testing Web Page\n"
     ]
    }
   ],
   "source": [
    "print(data.title.name)\n",
    "print(data.title.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be0b2764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class': ['abc'], 'id': 'first_para'}\n"
     ]
    }
   ],
   "source": [
    "print(data.p.attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e52818af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'first_para'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.p['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ab7cf16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'first_para'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.p.get('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1d4db505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"   Testing Web Page    Web Scraping             Let's start learning                            Web Scraping                          You can read more about BeautifulSoup from  here     Coding Ninjas   \""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cf5fa0b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"abc\" id=\"first_para\">            Let's start learning            <b>                Web Scraping            </b> </p>,\n",
       " <p id=\"def\">            You can read more about BeautifulSoup from <a '=\"\" href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\"> here </a> </p>,\n",
       " <p class=\"abc\"> <a '=\"\" href=\"https://codingninjas.in/\"> Coding Ninjas </a> </p>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.find_all('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18e65a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"abc\" id=\"first_para\">            Let's start learning            <b>                Web Scraping            </b> </p>,\n",
       " <p id=\"def\">            You can read more about BeautifulSoup from <a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\"> here </a> </p>,\n",
       " <a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\"> here </a>,\n",
       " <p class=\"abc\"> <a href=\"https://codingninjas.in/\"> Coding Ninjas </a> </p>,\n",
       " <a href=\"https://codingninjas.in/\"> Coding Ninjas </a>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.find_all(['p','a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "37f2bf76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<html> <head> <title> Testing Web Page</title> </head> <body> <h1> Web Scraping</h1> <p class=\"abc\" id=\"first_para\">            Let's start learning            <b>                Web Scraping            </b> </p> <p id=\"def\">            You can read more about BeautifulSoup from <a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\"> here </a> </p> <p class=\"abc\"> <a href=\"https://codingninjas.in/\"> Coding Ninjas </a> </p> </body></html>,\n",
       " <head> <title> Testing Web Page</title> </head>,\n",
       " <title> Testing Web Page</title>,\n",
       " <body> <h1> Web Scraping</h1> <p class=\"abc\" id=\"first_para\">            Let's start learning            <b>                Web Scraping            </b> </p> <p id=\"def\">            You can read more about BeautifulSoup from <a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\"> here </a> </p> <p class=\"abc\"> <a href=\"https://codingninjas.in/\"> Coding Ninjas </a> </p> </body>,\n",
       " <h1> Web Scraping</h1>,\n",
       " <p class=\"abc\" id=\"first_para\">            Let's start learning            <b>                Web Scraping            </b> </p>,\n",
       " <b>                Web Scraping            </b>,\n",
       " <p id=\"def\">            You can read more about BeautifulSoup from <a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\"> here </a> </p>,\n",
       " <a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\"> here </a>,\n",
       " <p class=\"abc\"> <a href=\"https://codingninjas.in/\"> Coding Ninjas </a> </p>,\n",
       " <a href=\"https://codingninjas.in/\"> Coding Ninjas </a>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.find_all(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9e58c357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"abc\" id=\"first_para\">            Let's start learning            <b>                Web Scraping            </b> </p>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.find_all(id='first_para')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2dd8b328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"abc\" id=\"first_para\">            Let's start learning            <b>                Web Scraping            </b> </p>,\n",
       " <p class=\"abc\"> <a href=\"https://codingninjas.in/\"> Coding Ninjas </a> </p>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.find_all(class_='abc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8efb9022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "li = data.find_all('p')\n",
    "for i in li:\n",
    "    print(i.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5a3f0def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Let's start learning\", 'Web Scraping']\n",
      "['You can read more about BeautifulSoup from', 'here']\n",
      "['Coding Ninjas']\n"
     ]
    }
   ],
   "source": [
    "li = data.find_all('p')\n",
    "for i in li:\n",
    "    print(list(i.stripped_strings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "57a64908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " <head> <title> Testing Web Page</title> </head>,\n",
       " ' ',\n",
       " <body> <h1> Web Scraping</h1> <p class=\"abc\" id=\"first_para\">            Let's start learning            <b>                Web Scraping            </b> </p> <p id=\"def\">            You can read more about BeautifulSoup from <a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\"> here </a> </p> <p class=\"abc\"> <a href=\"https://codingninjas.in/\"> Coding Ninjas </a> </p> </body>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.html.contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6c0e9a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "<head> <title> Testing Web Page</title> </head>\n",
      " \n",
      "<body> <h1> Web Scraping</h1> <p class=\"abc\" id=\"first_para\">            Let's start learning            <b>                Web Scraping            </b> </p> <p id=\"def\">            You can read more about BeautifulSoup from <a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\"> here </a> </p> <p class=\"abc\"> <a href=\"https://codingninjas.in/\"> Coding Ninjas </a> </p> </body>\n"
     ]
    }
   ],
   "source": [
    "li = data.html.children\n",
    "for i in li:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8d48e178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "[' ', <head> <title> Testing Web Page</title> </head>, ' ', <title> Testing Web Page</title>, ' Testing Web Page', ' ', ' ', <body> <h1> Web Scraping</h1> <p class=\"abc\" id=\"first_para\">            Let's start learning            <b>                Web Scraping            </b> </p> <p id=\"def\">            You can read more about BeautifulSoup from <a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\"> here </a> </p> <p class=\"abc\"> <a href=\"https://codingninjas.in/\"> Coding Ninjas </a> </p> </body>, ' ', <h1> Web Scraping</h1>, ' Web Scraping', ' ', <p class=\"abc\" id=\"first_para\">            Let's start learning            <b>                Web Scraping            </b> </p>, \"            Let's start learning            \", <b>                Web Scraping            </b>, '                Web Scraping            ', ' ', ' ', <p id=\"def\">            You can read more about BeautifulSoup from <a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\"> here </a> </p>, '            You can read more about BeautifulSoup from ', <a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\"> here </a>, ' here ', ' ', ' ', <p class=\"abc\"> <a href=\"https://codingninjas.in/\"> Coding Ninjas </a> </p>, ' ', <a href=\"https://codingninjas.in/\"> Coding Ninjas </a>, ' Coding Ninjas ', ' ', ' ']\n"
     ]
    }
   ],
   "source": [
    "desc = list(data.html.descendants)\n",
    "print(len(desc))\n",
    "print(desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a012cb",
   "metadata": {},
   "source": [
    "# First Web Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "88064960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<header>\n",
       "<title>The World Wide Web project</title>\n",
       "<nextid n=\"55\"/>\n",
       "</header>\n",
       "<body>\n",
       "<h1>World Wide Web</h1>The WorldWideWeb (W3) is a wide-area<a href=\"WhatIs.html\" name=\"0\">\n",
       "hypermedia</a> information retrieval\n",
       "initiative aiming to give universal\n",
       "access to a large universe of documents.<p>\n",
       "Everything there is online about\n",
       "W3 is linked directly or indirectly\n",
       "to this document, including an <a href=\"Summary.html\" name=\"24\">executive\n",
       "summary</a> of the project, <a href=\"Administration/Mailing/Overview.html\" name=\"29\">Mailing lists</a>\n",
       ", <a href=\"Policy.html\" name=\"30\">Policy</a> , November's  <a href=\"News/9211.html\" name=\"34\">W3  news</a> ,\n",
       "<a href=\"FAQ/List.html\" name=\"41\">Frequently Asked Questions</a> .\n",
       "<dl>\n",
       "<dt><a href=\"../DataSources/Top.html\" name=\"44\">What's out there?</a>\n",
       "<dd> Pointers to the\n",
       "world's online information,<a href=\"../DataSources/bySubject/Overview.html\" name=\"45\"> subjects</a>\n",
       ", <a href=\"../DataSources/WWW/Servers.html\" name=\"z54\">W3 servers</a>, etc.\n",
       "<dt><a href=\"Help.html\" name=\"46\">Help</a>\n",
       "<dd> on the browser you are using\n",
       "<dt><a href=\"Status.html\" name=\"13\">Software Products</a>\n",
       "<dd> A list of W3 project\n",
       "components and their current state.\n",
       "(e.g. <a href=\"LineMode/Browser.html\" name=\"27\">Line Mode</a> ,X11 <a href=\"Status.html#35\" name=\"35\">Viola</a> ,  <a href=\"NeXT/WorldWideWeb.html\" name=\"26\">NeXTStep</a>\n",
       ", <a href=\"Daemon/Overview.html\" name=\"25\">Servers</a> , <a href=\"Tools/Overview.html\" name=\"51\">Tools</a> ,<a href=\"MailRobot/Overview.html\" name=\"53\"> Mail robot</a> ,<a href=\"Status.html#57\" name=\"52\">\n",
       "Library</a> )\n",
       "<dt><a href=\"Technical.html\" name=\"47\">Technical</a>\n",
       "<dd> Details of protocols, formats,\n",
       "program internals etc\n",
       "<dt><a href=\"Bibliography.html\" name=\"40\">Bibliography</a>\n",
       "<dd> Paper documentation\n",
       "on  W3 and references.\n",
       "<dt><a href=\"People.html\" name=\"14\">People</a>\n",
       "<dd> A list of some people involved\n",
       "in the project.\n",
       "<dt><a href=\"History.html\" name=\"15\">History</a>\n",
       "<dd> A summary of the history\n",
       "of the project.\n",
       "<dt><a href=\"Helping.html\" name=\"37\">How can I help</a> ?\n",
       "<dd> If you would like\n",
       "to support the web..\n",
       "<dt><a href=\"../README.html\" name=\"48\">Getting code</a>\n",
       "<dd> Getting the code by<a href=\"LineMode/Defaults/Distribution.html\" name=\"49\">\n",
       "anonymous FTP</a> , etc.\n",
       "</dd></dt></dd></dt></dd></dt></dd></dt></dd></dt></dd></dt></dd></dt></dd></dt></dd></dt></dl>\n",
       "</p></body>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get('http://info.cern.ch/hypertext/WWW/TheProject.html')\n",
    "html = response.text\n",
    "data = BeautifulSoup(html,'html.parser')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3e63ac91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'World Wide Web'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.h1.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7b2ab141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The World Wide Web project'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.title.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2f25d043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThe World Wide Web project\\n\\n\\n\\nWorld Wide WebThe WorldWideWeb (W3) is a wide-area\\nhypermedia information retrieval\\ninitiative aiming to give universal\\naccess to a large universe of documents.\\nEverything there is online about\\nW3 is linked directly or indirectly\\nto this document, including an executive\\nsummary of the project, Mailing lists\\n, Policy , November's  W3  news ,\\nFrequently Asked Questions .\\n\\nWhat's out there?\\n Pointers to the\\nworld's online information, subjects\\n, W3 servers, etc.\\nHelp\\n on the browser you are using\\nSoftware Products\\n A list of W3 project\\ncomponents and their current state.\\n(e.g. Line Mode ,X11 Viola ,  NeXTStep\\n, Servers , Tools , Mail robot ,\\nLibrary )\\nTechnical\\n Details of protocols, formats,\\nprogram internals etc\\nBibliography\\n Paper documentation\\non  W3 and references.\\nPeople\\n A list of some people involved\\nin the project.\\nHistory\\n A summary of the history\\nof the project.\\nHow can I help ?\\n If you would like\\nto support the web..\\nGetting code\\n Getting the code by\\nanonymous FTP , etc.\\n\\n\\n\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5523aeb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WhatIs.html'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.a['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1a7203fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "hypermedia\n",
      "executive\n",
      "summary\n",
      "Mailing lists\n",
      "Policy\n",
      "W3  news\n",
      "Frequently Asked Questions\n",
      "What's out there?\n",
      " subjects\n",
      "W3 servers\n",
      "Help\n",
      "Software Products\n",
      "Line Mode\n",
      "Viola\n",
      "NeXTStep\n",
      "Servers\n",
      "Tools\n",
      " Mail robot\n",
      "\n",
      "Library\n",
      "Technical\n",
      "Bibliography\n",
      "People\n",
      "History\n",
      "How can I help\n",
      "Getting code\n",
      "\n",
      "anonymous FTP\n"
     ]
    }
   ],
   "source": [
    "li = data.find_all('a')\n",
    "for i in li:\n",
    "    print(i.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "47bbff52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"../DataSources/Top.html\" name=\"44\">What's out there?</a>,\n",
       " <a href=\"../DataSources/bySubject/Overview.html\" name=\"45\"> subjects</a>,\n",
       " <a href=\"../DataSources/WWW/Servers.html\" name=\"z54\">W3 servers</a>,\n",
       " <a href=\"Help.html\" name=\"46\">Help</a>,\n",
       " <a href=\"Status.html\" name=\"13\">Software Products</a>,\n",
       " <a href=\"LineMode/Browser.html\" name=\"27\">Line Mode</a>,\n",
       " <a href=\"Status.html#35\" name=\"35\">Viola</a>,\n",
       " <a href=\"NeXT/WorldWideWeb.html\" name=\"26\">NeXTStep</a>,\n",
       " <a href=\"Daemon/Overview.html\" name=\"25\">Servers</a>,\n",
       " <a href=\"Tools/Overview.html\" name=\"51\">Tools</a>,\n",
       " <a href=\"MailRobot/Overview.html\" name=\"53\"> Mail robot</a>,\n",
       " <a href=\"Status.html#57\" name=\"52\">\n",
       " Library</a>,\n",
       " <a href=\"Technical.html\" name=\"47\">Technical</a>,\n",
       " <a href=\"Bibliography.html\" name=\"40\">Bibliography</a>,\n",
       " <a href=\"People.html\" name=\"14\">People</a>,\n",
       " <a href=\"History.html\" name=\"15\">History</a>,\n",
       " <a href=\"Helping.html\" name=\"37\">How can I help</a>,\n",
       " <a href=\"../README.html\" name=\"48\">Getting code</a>,\n",
       " <a href=\"LineMode/Defaults/Distribution.html\" name=\"49\">\n",
       " anonymous FTP</a>]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li = data.dl.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0279b500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's out there?\n",
      "Help\n",
      "Software Products\n",
      "Technical\n",
      "Bibliography\n",
      "People\n",
      "History\n",
      "How can I help\n",
      "Getting code\n"
     ]
    }
   ],
   "source": [
    "li = data.dl.find_all('dt')\n",
    "for i in li:\n",
    "    print(i.a.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e0f6a8",
   "metadata": {},
   "source": [
    "# Books to scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f0f03079",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://books.toscrape.com/')\n",
    "data = BeautifulSoup(response.text,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "836661fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    All products | Books to Scrape - Sandbox\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data.title.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c81d4c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a href=\"index.html\">Books to Scrape</a>\n"
     ]
    }
   ],
   "source": [
    "print(data.a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1dd9a1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Light in the Attic\n",
      "https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\n"
     ]
    }
   ],
   "source": [
    "book = data.find(class_=\"product_pod\")\n",
    "print(book.h3.a[\"title\"])\n",
    "print('https://books.toscrape.com/' + book.h3.a['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "50acf232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\n",
      "https://books.toscrape.com/catalogue/tipping-the-velvet_999/index.html\n",
      "https://books.toscrape.com/catalogue/soumission_998/index.html\n",
      "https://books.toscrape.com/catalogue/sharp-objects_997/index.html\n",
      "https://books.toscrape.com/catalogue/sapiens-a-brief-history-of-humankind_996/index.html\n",
      "https://books.toscrape.com/catalogue/the-requiem-red_995/index.html\n",
      "https://books.toscrape.com/catalogue/the-dirty-little-secrets-of-getting-your-dream-job_994/index.html\n",
      "https://books.toscrape.com/catalogue/the-coming-woman-a-novel-based-on-the-life-of-the-infamous-feminist-victoria-woodhull_993/index.html\n",
      "https://books.toscrape.com/catalogue/the-boys-in-the-boat-nine-americans-and-their-epic-quest-for-gold-at-the-1936-berlin-olympics_992/index.html\n",
      "https://books.toscrape.com/catalogue/the-black-maria_991/index.html\n",
      "https://books.toscrape.com/catalogue/starving-hearts-triangular-trade-trilogy-1_990/index.html\n",
      "https://books.toscrape.com/catalogue/shakespeares-sonnets_989/index.html\n",
      "https://books.toscrape.com/catalogue/set-me-free_988/index.html\n",
      "https://books.toscrape.com/catalogue/scott-pilgrims-precious-little-life-scott-pilgrim-1_987/index.html\n",
      "https://books.toscrape.com/catalogue/rip-it-up-and-start-again_986/index.html\n",
      "https://books.toscrape.com/catalogue/our-band-could-be-your-life-scenes-from-the-american-indie-underground-1981-1991_985/index.html\n",
      "https://books.toscrape.com/catalogue/olio_984/index.html\n",
      "https://books.toscrape.com/catalogue/mesaerion-the-best-science-fiction-stories-1800-1849_983/index.html\n",
      "https://books.toscrape.com/catalogue/libertarianism-for-beginners_982/index.html\n",
      "https://books.toscrape.com/catalogue/its-only-the-himalayas_981/index.html\n"
     ]
    }
   ],
   "source": [
    "books = data.find_all(class_=\"product_pod\")\n",
    "for book in books:\n",
    "    print('https://books.toscrape.com/' + book.h3.a[\"href\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf890d6",
   "metadata": {},
   "source": [
    "# Problem 1 : Print Body tag\n",
    "You are given the HTML content of a webpage, your task is to :\n",
    "Print all the contents of the body tag(including body tag)\n",
    "\n",
    "NOTE : You are provided the HTML content inside variable html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd162d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<body><h1> About Us </h1><div class=\"first_div\"><p>Coding Ninjas Website</p><a href=\"https://www.codingninjas.in/\">Link to Coding Ninjas Website</a><ul><li>This</li><li>is</li><li>an</li><li>unordered</li><li>list.</li></ul></div><p id=\"template_p\">This is a template paragraph tag</p><a href=\"https://www.facebook.com/codingninjas/\">This is the link of our Facebook Page</a></body>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = '<!DOCTYPE html><html><head><title>Learning Beautiful Soup</title></head>\\\n",
    "<body><h1> About Us </h1><div class = \"first_div\"><p>Coding Ninjas Website</p>\\\n",
    "<a href=\"https://www.codingninjas.in/\">Link to Coding Ninjas Website</a>\\\n",
    "<ul><li>This</li><li>is</li><li>an</li><li>unordered</li><li>list.</li></ul>\\\n",
    "</div><p id = \"template_p\">This is a template paragraph tag</p>\\\n",
    "<a href = \"https://www.facebook.com/codingninjas/\">\\\n",
    "This is the link of our Facebook Page</a></body></html>'\n",
    "\n",
    "data = BeautifulSoup(html,'html.parser')\n",
    "data.body"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fedd51",
   "metadata": {},
   "source": [
    "# Problem 2 : Attributes of div tag\n",
    "You are given the HTML content of a webpage, your task is to :\n",
    "Print the name of all attributes of first div tag of the page\n",
    "\n",
    "NOTE : You are provided the HTML content inside variable html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69639bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class': ['first_div']}\n",
      "class\n"
     ]
    }
   ],
   "source": [
    "data = BeautifulSoup(html,'html.parser')\n",
    "print(data.div.attrs)\n",
    "for i in data.div.attrs:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157865cd",
   "metadata": {},
   "source": [
    "# Problem 3 : Strings of li\n",
    "You are given the HTML content of a webpage, your task is to :\n",
    "Print the strings(only text without tag names) of all li tags separated by a space\n",
    "\n",
    "NOTE : You are provided the HTML content inside variable html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ebf87cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an unordered list. "
     ]
    }
   ],
   "source": [
    "data = BeautifulSoup(html,'html.parser')\n",
    "lists = data.find_all('li')\n",
    "for l in lists:\n",
    "    print(l.string,end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256e2b2e",
   "metadata": {},
   "source": [
    "# Problem 4 : href of A tag\n",
    "You are given the HTML content of a webpage, your task is to : Print the href of all the tags on the page in different lines\n",
    "\n",
    "NOTE : You are provided the HTML content inside variable html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83982b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.codingninjas.in/\n",
      "https://www.facebook.com/codingninjas/\n"
     ]
    }
   ],
   "source": [
    "data = BeautifulSoup(html,'html.parser')\n",
    "anchor = data.find_all('a')\n",
    "for a in anchor:\n",
    "    print(a.attrs['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e698c8f",
   "metadata": {},
   "source": [
    "# Problem 5 : Descendants and children\n",
    "You are given the HTML content of a webpage, your task is to :\n",
    "Print the difference between the number of descendants and the number of children of the html tag\n",
    "\n",
    "NOTE : You are provided the HTML content inside variable html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c772314d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "html = '<!DOCTYPE html><html><head><title>Navigate Parse Tree</title></head>\\\n",
    "<body><h1>This is your Assignment</h1><a href = \"https://www.google.com\">This is a link that will take you to Google</a>\\\n",
    "<ul><li><p> This question is given to test your knowledge of <b>Web Scraping</b></p>\\\n",
    "<p>Web scraping is a term used to describe the use of a program or algorithm to extract and process large amounts of data from the web.</p></li>\\\n",
    "<li id = \"li2\">This is an li tag given to you for scraping</li>\\\n",
    "<li>This li tag gives you the various ways to get data from a website\\\n",
    "<ol><li class = \"list_or\">Using API of the website</li><li>Scrape data using BeautifulSoup</li><li>Scrape data using Selenium</li>\\\n",
    "<li>Scrape data using Scrapy</li></ol></li>\\\n",
    "<li class = \"list_or\"><a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\">\\\n",
    "Clicking on this takes you to the documentation of BeautifulSoup</a>\\\n",
    "<a href=\"https://selenium-python.readthedocs.io/\" id=\"anchor\">Clicking on this takes you to the documentation of Selenium</a>\\\n",
    "</li></ul></body></html>'\n",
    "\n",
    "data = BeautifulSoup(html,'html.parser')\n",
    "desc = list(data.html.descendants)\n",
    "child = list(data.html.children)\n",
    "print(len(desc) - len(child))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9754b491",
   "metadata": {},
   "source": [
    "# Problem 6 : Name of tags with ID\n",
    "You are given the HTML content of a webpage, your task is to :\n",
    "Print the name of all the tags in different lines that have an id attribute.\n",
    "\n",
    "NOTE : You are provided the HTML content inside variable html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a497be3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "li\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "data = BeautifulSoup(html,'html.parser')\n",
    "ids = data.find_all(id=True)\n",
    "for i in ids:\n",
    "    print(i.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d8077d",
   "metadata": {},
   "source": [
    "# Problem 7 : Next Sibling\n",
    "You are given the HTML content of a webpage, your task is to :\n",
    "Print all content of the next siblings of the tag that have id as “li2”(in different lines)\n",
    "\n",
    "NOTE : Content includes the complete html of tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bc155869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<li>This li tag gives you the various ways to get data from a website<ol><li class=\"list_or\">Using API of the website</li><li>Scrape data using BeautifulSoup</li><li>Scrape data using Selenium</li><li>Scrape data using Scrapy</li></ol></li>\n",
      "<li class=\"list_or\"><a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\">Clicking on this takes you to the documentation of BeautifulSoup</a><a href=\"https://selenium-python.readthedocs.io/\" id=\"anchor\">Clicking on this takes you to the documentation of Selenium</a></li>\n"
     ]
    }
   ],
   "source": [
    "data = BeautifulSoup(html,'html.parser')\n",
    "l = data.find(id=\"li2\").next_siblings\n",
    "for i in l:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14878399",
   "metadata": {},
   "source": [
    "# Problem 8 : Parents of title\n",
    "You are given the HTML content of a webpage, your task is to :\n",
    "Print content of all the parents of the title tag(linewise)\n",
    "\n",
    "NOTE : Content includes the whole HTML of the tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f65d6a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<head><title>Navigate Parse Tree</title></head>\n",
      "<html><head><title>Navigate Parse Tree</title></head><body><h1>This is your Assignment</h1><a href=\"https://www.google.com\">This is a link that will take you to Google</a><ul><li><p> This question is given to test your knowledge of <b>Web Scraping</b></p><p>Web scraping is a term used to describe the use of a program or algorithm to extract and process large amounts of data from the web.</p></li><li id=\"li2\">This is an li tag given to you for scraping</li><li>This li tag gives you the various ways to get data from a website<ol><li class=\"list_or\">Using API of the website</li><li>Scrape data using BeautifulSoup</li><li>Scrape data using Selenium</li><li>Scrape data using Scrapy</li></ol></li><li class=\"list_or\"><a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\">Clicking on this takes you to the documentation of BeautifulSoup</a><a href=\"https://selenium-python.readthedocs.io/\" id=\"anchor\">Clicking on this takes you to the documentation of Selenium</a></li></ul></body></html>\n",
      "<!DOCTYPE html>\n",
      "<html><head><title>Navigate Parse Tree</title></head><body><h1>This is your Assignment</h1><a href=\"https://www.google.com\">This is a link that will take you to Google</a><ul><li><p> This question is given to test your knowledge of <b>Web Scraping</b></p><p>Web scraping is a term used to describe the use of a program or algorithm to extract and process large amounts of data from the web.</p></li><li id=\"li2\">This is an li tag given to you for scraping</li><li>This li tag gives you the various ways to get data from a website<ol><li class=\"list_or\">Using API of the website</li><li>Scrape data using BeautifulSoup</li><li>Scrape data using Selenium</li><li>Scrape data using Scrapy</li></ol></li><li class=\"list_or\"><a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\">Clicking on this takes you to the documentation of BeautifulSoup</a><a href=\"https://selenium-python.readthedocs.io/\" id=\"anchor\">Clicking on this takes you to the documentation of Selenium</a></li></ul></body></html>\n"
     ]
    }
   ],
   "source": [
    "data = BeautifulSoup(html,'html.parser')\n",
    "for i in data.title.parents:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793d50b0",
   "metadata": {},
   "source": [
    "# Problem 9 : Next Element\n",
    "You are given the HTML content of a webpage, your task is to :\n",
    "Print the string which is present inside the second anchor tag using BeautifulSoup's next_element property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cc245398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clicking on this takes you to the documentation of BeautifulSoup\n"
     ]
    }
   ],
   "source": [
    "data = BeautifulSoup(html,'html.parser')\n",
    "sec_anchor = data.find_all('a')[1]\n",
    "print(sec_anchor.next_element)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce32bc7",
   "metadata": {},
   "source": [
    "# Problem 10 : Book Names from First Page\n",
    "Print the title of all 20 books which are present on first page of this https://books.toscrape.com/ website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3efd155a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Light in the Attic\n",
      "Tipping the Velvet\n",
      "Soumission\n",
      "Sharp Objects\n",
      "Sapiens: A Brief History of Humankind\n",
      "The Requiem Red\n",
      "The Dirty Little Secrets of Getting Your Dream Job\n",
      "The Coming Woman: A Novel Based on the Life of the Infamous Feminist, Victoria Woodhull\n",
      "The Boys in the Boat: Nine Americans and Their Epic Quest for Gold at the 1936 Berlin Olympics\n",
      "The Black Maria\n",
      "Starving Hearts (Triangular Trade Trilogy, #1)\n",
      "Shakespeare's Sonnets\n",
      "Set Me Free\n",
      "Scott Pilgrim's Precious Little Life (Scott Pilgrim #1)\n",
      "Rip it Up and Start Again\n",
      "Our Band Could Be Your Life: Scenes from the American Indie Underground, 1981-1991\n",
      "Olio\n",
      "Mesaerion: The Best Science Fiction Stories 1800-1849\n",
      "Libertarianism for Beginners\n",
      "It's Only the Himalayas\n"
     ]
    }
   ],
   "source": [
    "response = requests.get('https://books.toscrape.com/')\n",
    "data = BeautifulSoup(response.text,'html.parser')\n",
    "books = data.find_all(class_='product_pod')\n",
    "for book in books:\n",
    "    print(book.h3.a['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dc48cc",
   "metadata": {},
   "source": [
    "# Problem 11 : All Categories\n",
    "Print the name of all categories which are present this https://books.toscrape.com/ website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e4c2ed4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Travel\n",
      "Mystery\n",
      "Historical Fiction\n",
      "Sequential Art\n",
      "Classics\n",
      "Philosophy\n",
      "Romance\n",
      "Womens Fiction\n",
      "Fiction\n",
      "Childrens\n",
      "Religion\n",
      "Nonfiction\n",
      "Music\n",
      "Default\n",
      "Science Fiction\n",
      "Sports and Games\n",
      "Add a comment\n",
      "Fantasy\n",
      "New Adult\n",
      "Young Adult\n",
      "Science\n",
      "Poetry\n",
      "Paranormal\n",
      "Art\n",
      "Psychology\n",
      "Autobiography\n",
      "Parenting\n",
      "Adult Fiction\n",
      "Humor\n",
      "Horror\n",
      "History\n",
      "Food and Drink\n",
      "Christian Fiction\n",
      "Business\n",
      "Biography\n",
      "Thriller\n",
      "Contemporary\n",
      "Spirituality\n",
      "Academic\n",
      "Self Help\n",
      "Historical\n",
      "Christian\n",
      "Suspense\n",
      "Short Stories\n",
      "Novels\n",
      "Health\n",
      "Politics\n",
      "Cultural\n",
      "Erotica\n",
      "Crime\n"
     ]
    }
   ],
   "source": [
    "response = requests.get('https://books.toscrape.com/')\n",
    "data = BeautifulSoup(response.text,'html.parser')\n",
    "category = data.find(class_='side_categories')\n",
    "categories = category.find_all('a')\n",
    "for book in categories:\n",
    "    if book.string.strip()!='Books':\n",
    "        print(book.string.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a35eeb3",
   "metadata": {},
   "source": [
    "# Link of all the pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c12b2768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://books.toscrape.com/catalogue/page-2.html\n",
      "https://books.toscrape.com/catalogue/page-3.html\n",
      "https://books.toscrape.com/catalogue/page-4.html\n",
      "https://books.toscrape.com/catalogue/page-5.html\n",
      "https://books.toscrape.com/catalogue/page-6.html\n",
      "https://books.toscrape.com/catalogue/page-7.html\n",
      "https://books.toscrape.com/catalogue/page-8.html\n",
      "https://books.toscrape.com/catalogue/page-9.html\n",
      "https://books.toscrape.com/catalogue/page-10.html\n",
      "https://books.toscrape.com/catalogue/page-11.html\n",
      "https://books.toscrape.com/catalogue/page-12.html\n",
      "https://books.toscrape.com/catalogue/page-13.html\n",
      "https://books.toscrape.com/catalogue/page-14.html\n",
      "https://books.toscrape.com/catalogue/page-15.html\n",
      "https://books.toscrape.com/catalogue/page-16.html\n",
      "https://books.toscrape.com/catalogue/page-17.html\n",
      "https://books.toscrape.com/catalogue/page-18.html\n",
      "https://books.toscrape.com/catalogue/page-19.html\n",
      "https://books.toscrape.com/catalogue/page-20.html\n",
      "https://books.toscrape.com/catalogue/page-21.html\n",
      "https://books.toscrape.com/catalogue/page-22.html\n",
      "https://books.toscrape.com/catalogue/page-23.html\n",
      "https://books.toscrape.com/catalogue/page-24.html\n",
      "https://books.toscrape.com/catalogue/page-25.html\n",
      "https://books.toscrape.com/catalogue/page-26.html\n",
      "https://books.toscrape.com/catalogue/page-27.html\n",
      "https://books.toscrape.com/catalogue/page-28.html\n",
      "https://books.toscrape.com/catalogue/page-29.html\n",
      "https://books.toscrape.com/catalogue/page-30.html\n",
      "https://books.toscrape.com/catalogue/page-31.html\n",
      "https://books.toscrape.com/catalogue/page-32.html\n",
      "https://books.toscrape.com/catalogue/page-33.html\n",
      "https://books.toscrape.com/catalogue/page-34.html\n",
      "https://books.toscrape.com/catalogue/page-35.html\n",
      "https://books.toscrape.com/catalogue/page-36.html\n",
      "https://books.toscrape.com/catalogue/page-37.html\n",
      "https://books.toscrape.com/catalogue/page-38.html\n",
      "https://books.toscrape.com/catalogue/page-39.html\n",
      "https://books.toscrape.com/catalogue/page-40.html\n",
      "https://books.toscrape.com/catalogue/page-41.html\n",
      "https://books.toscrape.com/catalogue/page-42.html\n",
      "https://books.toscrape.com/catalogue/page-43.html\n",
      "https://books.toscrape.com/catalogue/page-44.html\n",
      "https://books.toscrape.com/catalogue/page-45.html\n",
      "https://books.toscrape.com/catalogue/page-46.html\n",
      "https://books.toscrape.com/catalogue/page-47.html\n",
      "https://books.toscrape.com/catalogue/page-48.html\n",
      "https://books.toscrape.com/catalogue/page-49.html\n",
      "https://books.toscrape.com/catalogue/page-50.html\n"
     ]
    }
   ],
   "source": [
    "url = 'https://books.toscrape.com/catalogue/'\n",
    "response = requests.get('https://books.toscrape.com/catalogue/page-1.html')\n",
    "\n",
    "while response.status_code==200:\n",
    "    current_page = BeautifulSoup(response.text,'html.parser')\n",
    "    next_page = current_page.find(class_='next')\n",
    "    if next_page is None:\n",
    "        break\n",
    "    next_page_url = url + next_page.a['href']\n",
    "    print(next_page_url)\n",
    "    response = requests.get(next_page_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee9655c",
   "metadata": {},
   "source": [
    "# Store data in csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b500d40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://books.toscrape.com/'\n",
    "response = requests.get(url)\n",
    "data = BeautifulSoup(response.text,'html.parser')\n",
    "books = data.find_all(class_='product_pod')\n",
    "\n",
    "books_1st_page = []\n",
    "for book in books:\n",
    "    book_url = url + book.a['href']\n",
    "    book_response = requests.get(book_url)\n",
    "    book_data = BeautifulSoup(book_response.text,'html.parser')\n",
    "    title = book_data.h1.string\n",
    "    price = book_data.find(class_='price_color').string\n",
    "    price = re.search('[\\d.]+',price).group()\n",
    "    qty = book_data.find(class_='instock availability').contents[-1].strip()\n",
    "    qty = re.search('\\d+',qty).group()\n",
    "    book_details = []\n",
    "    book_details.append(title)\n",
    "    book_details.append(price)\n",
    "    book_details.append(qty)\n",
    "    books_1st_page.append(book_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d497817",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(books_1st_page,columns=['Title','Price','Qty in stock'])\n",
    "df.to_csv('Books.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0502c666",
   "metadata": {},
   "source": [
    "# Problem 12 : All Book Names\n",
    "Print the title of all books which are present on first 10 pages of this https://books.toscrape.com/ website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31955078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Light in the Attic\n",
      "Tipping the Velvet\n",
      "Soumission\n",
      "Sharp Objects\n",
      "Sapiens: A Brief History of Humankind\n",
      "The Requiem Red\n",
      "The Dirty Little Secrets of Getting Your Dream Job\n",
      "The Coming Woman: A Novel Based on the Life of the Infamous Feminist, Victoria Woodhull\n",
      "The Boys in the Boat: Nine Americans and Their Epic Quest for Gold at the 1936 Berlin Olympics\n",
      "The Black Maria\n",
      "Starving Hearts (Triangular Trade Trilogy, #1)\n",
      "Shakespeare's Sonnets\n",
      "Set Me Free\n",
      "Scott Pilgrim's Precious Little Life (Scott Pilgrim #1)\n",
      "Rip it Up and Start Again\n",
      "Our Band Could Be Your Life: Scenes from the American Indie Underground, 1981-1991\n",
      "Olio\n",
      "Mesaerion: The Best Science Fiction Stories 1800-1849\n",
      "Libertarianism for Beginners\n",
      "It's Only the Himalayas\n",
      "In Her Wake\n",
      "How Music Works\n",
      "Foolproof Preserving: A Guide to Small Batch Jams, Jellies, Pickles, Condiments, and More: A Foolproof Guide to Making Small Batch Jams, Jellies, Pickles, Condiments, and More\n",
      "Chase Me (Paris Nights #2)\n",
      "Black Dust\n",
      "Birdsong: A Story in Pictures\n",
      "America's Cradle of Quarterbacks: Western Pennsylvania's Football Factory from Johnny Unitas to Joe Montana\n",
      "Aladdin and His Wonderful Lamp\n",
      "Worlds Elsewhere: Journeys Around Shakespeareâs Globe\n",
      "Wall and Piece\n",
      "The Four Agreements: A Practical Guide to Personal Freedom\n",
      "The Five Love Languages: How to Express Heartfelt Commitment to Your Mate\n",
      "The Elephant Tree\n",
      "The Bear and the Piano\n",
      "Sophie's World\n",
      "Penny Maybe\n",
      "Maude (1883-1993):She Grew Up with the country\n",
      "In a Dark, Dark Wood\n",
      "Behind Closed Doors\n",
      "You can't bury them all: Poems\n",
      "Slow States of Collapse: Poems\n",
      "Reasons to Stay Alive\n",
      "Private Paris (Private #10)\n",
      "#HigherSelfie: Wake Up Your Life. Free Your Soul. Find Your Tribe.\n",
      "Without Borders (Wanderlove #1)\n",
      "When We Collided\n",
      "We Love You, Charlie Freeman\n",
      "Untitled Collection: Sabbath Poems 2014\n",
      "Unseen City: The Majesty of Pigeons, the Discreet Charm of Snails & Other Wonders of the Urban Wilderness\n",
      "Unicorn Tracks\n",
      "Unbound: How Eight Technologies Made Us Human, Transformed Society, and Brought Our World to the Brink\n",
      "Tsubasa: WoRLD CHRoNiCLE 2 (Tsubasa WoRLD CHRoNiCLE #2)\n",
      "Throwing Rocks at the Google Bus: How Growth Became the Enemy of Prosperity\n",
      "This One Summer\n",
      "Thirst\n",
      "The Torch Is Passed: A Harding Family Story\n",
      "The Secret of Dreadwillow Carse\n",
      "The Pioneer Woman Cooks: Dinnertime: Comfort Classics, Freezer Food, 16-Minute Meals, and Other Delicious Ways to Solve Supper!\n",
      "The Past Never Ends\n",
      "The Natural History of Us (The Fine Art of Pretending #2)\n",
      "The Nameless City (The Nameless City #1)\n",
      "The Murder That Never Was (Forensic Instincts #5)\n",
      "The Most Perfect Thing: Inside (and Outside) a Bird's Egg\n",
      "The Mindfulness and Acceptance Workbook for Anxiety: A Guide to Breaking Free from Anxiety, Phobias, and Worry Using Acceptance and Commitment Therapy\n",
      "The Life-Changing Magic of Tidying Up: The Japanese Art of Decluttering and Organizing\n",
      "The Inefficiency Assassin: Time Management Tactics for Working Smarter, Not Longer\n",
      "The Gutsy Girl: Escapades for Your Life of Epic Adventure\n",
      "The Electric Pencil: Drawings from Inside State Hospital No. 3\n",
      "The Death of Humanity: and the Case for Life\n",
      "The Bulletproof Diet: Lose up to a Pound a Day, Reclaim Energy and Focus, Upgrade Your Life\n",
      "The Art Forger\n",
      "The Age of Genius: The Seventeenth Century and the Birth of the Modern Mind\n",
      "The Activist's Tao Te Ching: Ancient Advice for a Modern Revolution\n",
      "Spark Joy: An Illustrated Master Class on the Art of Organizing and Tidying Up\n",
      "Soul Reader\n",
      "Security\n",
      "Saga, Volume 6 (Saga (Collected Editions) #6)\n",
      "Saga, Volume 5 (Saga (Collected Editions) #5)\n",
      "Reskilling America: Learning to Labor in the Twenty-First Century\n",
      "Rat Queens, Vol. 3: Demons (Rat Queens (Collected Editions) #11-15)\n",
      "Princess Jellyfish 2-in-1 Omnibus, Vol. 01 (Princess Jellyfish 2-in-1 Omnibus #1)\n",
      "Princess Between Worlds (Wide-Awake Princess #5)\n",
      "Pop Gun War, Volume 1: Gift\n",
      "Political Suicide: Missteps, Peccadilloes, Bad Calls, Backroom Hijinx, Sordid Pasts, Rotten Breaks, and Just Plain Dumb Mistakes in the Annals of American Politics\n",
      "Patience\n",
      "Outcast, Vol. 1: A Darkness Surrounds Him (Outcast #1)\n",
      "orange: The Complete Collection 1 (orange: The Complete Collection #1)\n",
      "Online Marketing for Busy Authors: A Step-By-Step Guide\n",
      "On a Midnight Clear\n",
      "Obsidian (Lux #1)\n",
      "My Paris Kitchen: Recipes and Stories\n",
      "Masks and Shadows\n",
      "Mama Tried: Traditional Italian Cooking for the Screwed, Crude, Vegan, and Tattooed\n",
      "Lumberjanes, Vol. 2: Friendship to the Max (Lumberjanes #5-8)\n",
      "Lumberjanes, Vol. 1: Beware the Kitten Holy (Lumberjanes #1-4)\n",
      "Lumberjanes Vol. 3: A Terrible Plan (Lumberjanes #9-12)\n",
      "Layered: Baking, Building, and Styling Spectacular Cakes\n",
      "Judo: Seven Steps to Black Belt (an Introductory Guide for Beginners)\n",
      "Join\n",
      "In the Country We Love: My Family Divided\n",
      "Immunity: How Elie Metchnikoff Changed the Course of Modern Medicine\n",
      "I Hate Fairyland, Vol. 1: Madly Ever After (I Hate Fairyland (Compilations) #1-5)\n",
      "I am a Hero Omnibus Volume 1\n",
      "How to Be Miserable: 40 Strategies You Already Use\n",
      "Her Backup Boyfriend (The Sorensen Family #1)\n",
      "Giant Days, Vol. 2 (Giant Days #5-8)\n",
      "Forever and Forever: The Courtship of Henry Longfellow and Fanny Appleton\n",
      "First and First (Five Boroughs #3)\n",
      "Fifty Shades Darker (Fifty Shades #2)\n",
      "Everydata: The Misinformation Hidden in the Little Data You Consume Every Day\n",
      "Don't Be a Jerk: And Other Practical Advice from Dogen, Japan's Greatest Zen Master\n",
      "Danganronpa Volume 1\n",
      "Crown of Midnight (Throne of Glass #2)\n",
      "Codename Baboushka, Volume 1: The Conclave of Death\n",
      "Camp Midnight\n",
      "Call the Nurse: True Stories of a Country Nurse on a Scottish Isle\n",
      "Burning\n",
      "Bossypants\n",
      "Bitch Planet, Vol. 1: Extraordinary Machine (Bitch Planet (Collected Editions))\n",
      "Avatar: The Last Airbender: Smoke and Shadow, Part 3 (Smoke and Shadow #3)\n",
      "Algorithms to Live By: The Computer Science of Human Decisions\n",
      "A World of Flavor: Your Gluten Free Passport\n",
      "A Piece of Sky, a Grain of Rice: A Memoir in Four Meditations\n",
      "A Murder in Time\n",
      "A Flight of Arrows (The Pathfinders #2)\n",
      "A Fierce and Subtle Poison\n",
      "A Court of Thorns and Roses (A Court of Thorns and Roses #1)\n",
      "(Un)Qualified: How God Uses Broken People to Do Big Things\n",
      "You Are What You Love: The Spiritual Power of Habit\n",
      "William Shakespeare's Star Wars: Verily, A New Hope (William Shakespeare's Star Wars #4)\n",
      "Tuesday Nights in 1980\n",
      "Tracing Numbers on a Train\n",
      "Throne of Glass (Throne of Glass #1)\n",
      "Thomas Jefferson and the Tripoli Pirates: The Forgotten War That Changed American History\n",
      "Thirteen Reasons Why\n",
      "The White Cat and the Monk: A Retelling of the Poem âPangur BÃ¡nâ\n",
      "The Wedding Dress\n",
      "The Vacationers\n",
      "The Third Wave: An Entrepreneurâs Vision of the Future\n",
      "The Stranger\n",
      "The Shadow Hero (The Shadow Hero)\n",
      "The Secret (The Secret #1)\n",
      "The Regional Office Is Under Attack!\n",
      "The Psychopath Test: A Journey Through the Madness Industry\n",
      "The Project\n",
      "The Power of Now: A Guide to Spiritual Enlightenment\n",
      "The Omnivore's Dilemma: A Natural History of Four Meals\n",
      "The Nerdy Nummies Cookbook: Sweet Treats for the Geek in All of Us\n",
      "The Murder of Roger Ackroyd (Hercule Poirot #4)\n",
      "The Mistake (Off-Campus #2)\n",
      "The Matchmaker's Playbook (Wingmen Inc. #1)\n",
      "The Love and Lemons Cookbook: An Apple-to-Zucchini Celebration of Impromptu Cooking\n",
      "The Long Shadow of Small Ghosts: Murder and Memory in an American City\n",
      "The Kite Runner\n",
      "The House by the Lake\n",
      "The Glittering Court (The Glittering Court #1)\n",
      "The Girl on the Train\n",
      "The Genius of Birds\n",
      "The Emerald Mystery\n",
      "The Cookies & Cups Cookbook: 125+ sweet & savory recipes reminding you to Always Eat Dessert First\n",
      "The Bridge to Consciousness: I'm Writing the Bridge Between Science and Our Old and New Beliefs.\n",
      "The Artist's Way: A Spiritual Path to Higher Creativity\n",
      "The Art of War\n",
      "The Argonauts\n",
      "The 10% Entrepreneur: Live Your Startup Dream Without Quitting Your Day Job\n",
      "Suddenly in Love (Lake Haven #1)\n",
      "Something More Than This\n",
      "Soft Apocalypse\n",
      "So You've Been Publicly Shamed\n",
      "Shoe Dog: A Memoir by the Creator of NIKE\n",
      "Shobu Samurai, Project Aryoku (#3)\n",
      "Secrets and Lace (Fatal Hearts #1)\n",
      "Scarlett Epstein Hates It Here\n",
      "Romero and Juliet: A Tragic Tale of Love and Zombies\n",
      "Redeeming Love\n",
      "Poses for Artists Volume 1 - Dynamic and Sitting Poses: An Essential Reference for Figure Drawing and the Human Form\n",
      "Poems That Make Grown Women Cry\n",
      "Nightingale, Sing\n",
      "Night Sky with Exit Wounds\n",
      "Mrs. Houdini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modern Romance\n",
      "Miss Peregrineâs Home for Peculiar Children (Miss Peregrineâs Peculiar Children #1)\n",
      "Louisa: The Extraordinary Life of Mrs. Adams\n",
      "Little Red\n",
      "Library of Souls (Miss Peregrineâs Peculiar Children #3)\n",
      "Large Print Heart of the Pride\n",
      "I Had a Nice Time And Other Lies...: How to find love & sh*t like that\n",
      "Hollow City (Miss Peregrineâs Peculiar Children #2)\n",
      "Grumbles\n",
      "Full Moon over Noahâs Ark: An Odyssey to Mount Ararat and Beyond\n",
      "Frostbite (Vampire Academy #2)\n",
      "Follow You Home\n",
      "First Steps for New Christians (Print Edition)\n",
      "Finders Keepers (Bill Hodges Trilogy #2)\n",
      "Fables, Vol. 1: Legends in Exile (Fables #1)\n",
      "Eureka Trivia 6.0\n",
      "Drive: The Surprising Truth About What Motivates Us\n",
      "Done Rubbed Out (Reightman & Bailey #1)\n",
      "Doing It Over (Most Likely To #1)\n",
      "Deliciously Ella Every Day: Quick and Easy Recipes for Gluten-Free Snacks, Packed Lunches, and Simple Meals\n"
     ]
    }
   ],
   "source": [
    "url = 'https://books.toscrape.com/catalogue/'\n",
    "next_url = url + 'page-1.html'\n",
    "\n",
    "for i in range(10):\n",
    "    response = requests.get(next_url)\n",
    "    data = BeautifulSoup(response.text,'html.parser')\n",
    "    books = data.find_all(class_='product_pod')\n",
    "    for book in books:\n",
    "        print(book.h3.a['title'])\n",
    "    next_page = data.find(class_='next').a['href']\n",
    "    next_url = url + next_page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c292e3",
   "metadata": {},
   "source": [
    "# Problem 13 : Book Details\n",
    "Find and print the details of all books which are present on first 2 pages of this https://books.toscrape.com/ website.\n",
    "All details include - Title of the book, book page url, Price (in float, without any currency or extra symbol), and quantity in stock (in integer). Save all the details in a dataframe and print in the required format.\n",
    "\n",
    "Note: Remove the Trailing Zeros from price of book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cc23d1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Light in the Attic https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html 51.77 22\n",
      "Tipping the Velvet https://books.toscrape.com/catalogue/tipping-the-velvet_999/index.html 53.74 20\n",
      "Soumission https://books.toscrape.com/catalogue/soumission_998/index.html 50.10 20\n",
      "Sharp Objects https://books.toscrape.com/catalogue/sharp-objects_997/index.html 47.82 20\n",
      "Sapiens: A Brief History of Humankind https://books.toscrape.com/catalogue/sapiens-a-brief-history-of-humankind_996/index.html 54.23 20\n",
      "The Requiem Red https://books.toscrape.com/catalogue/the-requiem-red_995/index.html 22.65 19\n",
      "The Dirty Little Secrets of Getting Your Dream Job https://books.toscrape.com/catalogue/the-dirty-little-secrets-of-getting-your-dream-job_994/index.html 33.34 19\n",
      "The Coming Woman: A Novel Based on the Life of the Infamous Feminist, Victoria Woodhull https://books.toscrape.com/catalogue/the-coming-woman-a-novel-based-on-the-life-of-the-infamous-feminist-victoria-woodhull_993/index.html 17.93 19\n",
      "The Boys in the Boat: Nine Americans and Their Epic Quest for Gold at the 1936 Berlin Olympics https://books.toscrape.com/catalogue/the-boys-in-the-boat-nine-americans-and-their-epic-quest-for-gold-at-the-1936-berlin-olympics_992/index.html 22.60 19\n",
      "The Black Maria https://books.toscrape.com/catalogue/the-black-maria_991/index.html 52.15 19\n",
      "Starving Hearts (Triangular Trade Trilogy, #1) https://books.toscrape.com/catalogue/starving-hearts-triangular-trade-trilogy-1_990/index.html 13.99 19\n",
      "Shakespeare's Sonnets https://books.toscrape.com/catalogue/shakespeares-sonnets_989/index.html 20.66 19\n",
      "Set Me Free https://books.toscrape.com/catalogue/set-me-free_988/index.html 17.46 19\n",
      "Scott Pilgrim's Precious Little Life (Scott Pilgrim #1) https://books.toscrape.com/catalogue/scott-pilgrims-precious-little-life-scott-pilgrim-1_987/index.html 52.29 19\n",
      "Rip it Up and Start Again https://books.toscrape.com/catalogue/rip-it-up-and-start-again_986/index.html 35.02 19\n",
      "Our Band Could Be Your Life: Scenes from the American Indie Underground, 1981-1991 https://books.toscrape.com/catalogue/our-band-could-be-your-life-scenes-from-the-american-indie-underground-1981-1991_985/index.html 57.25 19\n",
      "Olio https://books.toscrape.com/catalogue/olio_984/index.html 23.88 19\n",
      "Mesaerion: The Best Science Fiction Stories 1800-1849 https://books.toscrape.com/catalogue/mesaerion-the-best-science-fiction-stories-1800-1849_983/index.html 37.59 19\n",
      "Libertarianism for Beginners https://books.toscrape.com/catalogue/libertarianism-for-beginners_982/index.html 51.33 19\n",
      "It's Only the Himalayas https://books.toscrape.com/catalogue/its-only-the-himalayas_981/index.html 45.17 19\n",
      "In Her Wake https://books.toscrape.com/catalogue/in-her-wake_980/index.html 12.84 19\n",
      "How Music Works https://books.toscrape.com/catalogue/how-music-works_979/index.html 37.32 19\n",
      "Foolproof Preserving: A Guide to Small Batch Jams, Jellies, Pickles, Condiments, and More: A Foolproof Guide to Making Small Batch Jams, Jellies, Pickles, Condiments, and More https://books.toscrape.com/catalogue/foolproof-preserving-a-guide-to-small-batch-jams-jellies-pickles-condiments-and-more-a-foolproof-guide-to-making-small-batch-jams-jellies-pickles-condiments-and-more_978/index.html 30.52 19\n",
      "Chase Me (Paris Nights #2) https://books.toscrape.com/catalogue/chase-me-paris-nights-2_977/index.html 25.27 19\n",
      "Black Dust https://books.toscrape.com/catalogue/black-dust_976/index.html 34.53 19\n",
      "Birdsong: A Story in Pictures https://books.toscrape.com/catalogue/birdsong-a-story-in-pictures_975/index.html 54.64 19\n",
      "America's Cradle of Quarterbacks: Western Pennsylvania's Football Factory from Johnny Unitas to Joe Montana https://books.toscrape.com/catalogue/americas-cradle-of-quarterbacks-western-pennsylvanias-football-factory-from-johnny-unitas-to-joe-montana_974/index.html 22.50 19\n",
      "Aladdin and His Wonderful Lamp https://books.toscrape.com/catalogue/aladdin-and-his-wonderful-lamp_973/index.html 53.13 19\n",
      "Worlds Elsewhere: Journeys Around Shakespeareâs Globe https://books.toscrape.com/catalogue/worlds-elsewhere-journeys-around-shakespeares-globe_972/index.html 40.30 18\n",
      "Wall and Piece https://books.toscrape.com/catalogue/wall-and-piece_971/index.html 44.18 18\n",
      "The Four Agreements: A Practical Guide to Personal Freedom https://books.toscrape.com/catalogue/the-four-agreements-a-practical-guide-to-personal-freedom_970/index.html 17.66 18\n",
      "The Five Love Languages: How to Express Heartfelt Commitment to Your Mate https://books.toscrape.com/catalogue/the-five-love-languages-how-to-express-heartfelt-commitment-to-your-mate_969/index.html 31.05 18\n",
      "The Elephant Tree https://books.toscrape.com/catalogue/the-elephant-tree_968/index.html 23.82 18\n",
      "The Bear and the Piano https://books.toscrape.com/catalogue/the-bear-and-the-piano_967/index.html 36.89 18\n",
      "Sophie's World https://books.toscrape.com/catalogue/sophies-world_966/index.html 15.94 18\n",
      "Penny Maybe https://books.toscrape.com/catalogue/penny-maybe_965/index.html 33.29 18\n",
      "Maude (1883-1993):She Grew Up with the country https://books.toscrape.com/catalogue/maude-1883-1993she-grew-up-with-the-country_964/index.html 18.02 18\n",
      "In a Dark, Dark Wood https://books.toscrape.com/catalogue/in-a-dark-dark-wood_963/index.html 19.63 18\n",
      "Behind Closed Doors https://books.toscrape.com/catalogue/behind-closed-doors_962/index.html 52.22 18\n",
      "You can't bury them all: Poems https://books.toscrape.com/catalogue/you-cant-bury-them-all-poems_961/index.html 33.63 17\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://books.toscrape.com/'\n",
    "response = requests.get('https://books.toscrape.com/')\n",
    "\n",
    "book_page = []\n",
    "for i in range(2):\n",
    "    data = BeautifulSoup(response.text,'html.parser')\n",
    "    books = data.find_all(class_='product_pod')\n",
    "    for book in books:\n",
    "        if 'catalogue' in book.h3.a['href']:\n",
    "            book_url = 'https://books.toscrape.com/' + book.h3.a['href']\n",
    "        else:\n",
    "            book_url = url + 'catalogue/' + book.h3.a['href']\n",
    "        details = requests.get(book_url)\n",
    "        book_data = BeautifulSoup(details.text,'html.parser')\n",
    "        title = book_data.h1.string\n",
    "        price = book_data.find(class_='price_color').string\n",
    "        price = re.search('[\\d.]+',price).group()\n",
    "        qty = book_data.find(class_='instock availability').contents[-1].strip()\n",
    "        qty = re.search('\\d+',qty).group()\n",
    "        print(title,book_url,price,qty)\n",
    "        book_details = []\n",
    "        book_details.append(title)\n",
    "        book_details.append(book_url)\n",
    "        book_details.append(price)\n",
    "        book_details.append(qty)\n",
    "        book_page.append(book_details)\n",
    "    next_url = url + data.find(class_='next').a['href']\n",
    "    response = requests.get(next_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4835939e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(book_page,columns=['Title','Book URL','Price','qty in stock'])\n",
    "df.to_csv('books.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2190dc58",
   "metadata": {},
   "source": [
    "# Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5b4067",
   "metadata": {},
   "source": [
    "# Problem 1 : Print the data of first 3 movies\n",
    "From this https://www.imdb.com/search/title/?release_date=2018&sort=num_votes,desc&page=1&ref_=adv_nxt link,\n",
    "Find and print the name and genre of the first 3 titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6778b3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avengers: Infinity War Action, Adventure, Sci-Fi\n",
      "Black Panther Action, Adventure, Sci-Fi\n",
      "Spider-Man: Into the Spider-Verse Animation, Action, Adventure\n"
     ]
    }
   ],
   "source": [
    "response = requests.get('https://www.imdb.com/search/title/?release_date=2018&sort=num_votes,desc&page=1&ref_=adv_nxt')\n",
    "data = BeautifulSoup(response.text,'html.parser')\n",
    "movies = data.find_all(class_='lister-item mode-advanced')\n",
    "count = 1\n",
    "for movie in movies:\n",
    "    if count>3:\n",
    "        break\n",
    "    print(movie.h3.a.string.strip(),movie.find(class_='genre').string.strip())\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabcafd6",
   "metadata": {},
   "source": [
    "# Problem 2 : titles with most votes\n",
    "Link to use https://www.imdb.com/search/title/?release_date=2018&sort=num_votes,desc&page=1&ref_=adv_nxt\n",
    "Print the names of movies with highest number of votes from year 2010 to 2014.\n",
    "\n",
    "Note : Print the titles line wise starting from year 2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4fc9046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inception\n",
      "Game of Thrones\n",
      "The Dark Knight Rises\n",
      "The Wolf of Wall Street\n",
      "Interstellar\n"
     ]
    }
   ],
   "source": [
    "urls = ['https://www.imdb.com/search/title/?release_date=2010&sort=num_votes,desc&page=1&ref_=adv_nxt','https://www.imdb.com/search/title/?release_date=2011&sort=num_votes,desc&page=1&ref_=adv_nxt','https://www.imdb.com/search/title/?release_date=2012&sort=num_votes,desc&page=1&ref_=adv_nxt','https://www.imdb.com/search/title/?release_date=2013&sort=num_votes,desc&page=1&ref_=adv_nxt','https://www.imdb.com/search/title/?release_date=2014&sort=num_votes,desc&page=1&ref_=adv_nxt']\n",
    "for url in urls:\n",
    "    response = requests.get(url)\n",
    "    data = BeautifulSoup(response.text,'html.parser')\n",
    "    title = data.find(class_='lister-item-content').h3.a.string\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bd170a",
   "metadata": {},
   "source": [
    "# Problem 3 : Title with maximum duration\n",
    "Link to use https://www.imdb.com/search/title/?release_date=2018&sort=num_votes,desc&page=1&ref_=adv_nxt\n",
    "Out of the first 250 titles with highest number of votes in 2018, find which title has the maximum duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "02c880eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Haunting of Hill House 572\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.imdb.com'\n",
    "next_url ='/search/title/?release_date=2018-01-01,2018-12-31&sort=num_votes,desc&start=1'\n",
    "\n",
    "dic = {}\n",
    "for i in range(5):\n",
    "    response = requests.get(url + next_url + '&ref_=adv_nxt')\n",
    "    data = BeautifulSoup(response.text,'html.parser')\n",
    "    movies = data.find_all(class_='lister-item-content')\n",
    "    for movie in movies:\n",
    "        title = movie.h3.a.string\n",
    "        duration_object = movie.p.find(class_='runtime')\n",
    "        if duration_object is None:\n",
    "            continue\n",
    "        duration = duration_object.string\n",
    "        dic[title] = int(duration.split(\" \")[0])\n",
    "    next_url = data.find(class_='lister-page-next next-page')['href']\n",
    "keys = list(dic.keys())\n",
    "values = list(dic.values())\n",
    "max_value_index = np.argmax(values)\n",
    "print(keys[max_value_index],np.max(values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1b3f17",
   "metadata": {},
   "source": [
    "# Problem 4 : Applications of AI\n",
    "From this website : https://en.wikipedia.org/wiki/Artificial_intelligence\n",
    "Find and print all applications of AI (As present in Contents of the page)\n",
    "\n",
    "Note : Print applications line wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "90bd6524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI and machine learning technology is used in most of the essential applications of the 2020s, including: \n",
      "search engines\n",
      " (such as \n",
      "Google Search\n",
      "),\n",
      "\n",
      "targeting online advertisements\n",
      ",\n",
      "[119]\n",
      "\n",
      "\n",
      "recommendation systems\n",
      " (offered by \n",
      "Netflix\n",
      ", \n",
      "YouTube\n",
      " or \n",
      "Amazon\n",
      "), \n",
      "driving \n",
      "internet traffic\n",
      ",\n",
      "[120]\n",
      "[121]\n",
      "\n",
      "\n",
      "targeted advertising\n",
      " (\n",
      "AdSense\n",
      ", \n",
      "Facebook\n",
      "), \n",
      "\n",
      "virtual assistants\n",
      " (such as \n",
      "Siri\n",
      " or \n",
      "Alexa\n",
      "),\n",
      "[122]\n",
      "\n",
      "\n",
      "autonomous vehicles\n",
      " (including \n",
      "drones\n",
      ", \n",
      "\n",
      "ADAS\n",
      " and \n",
      "self-driving cars\n",
      "), \n",
      "\n",
      "automatic language translation\n",
      " (\n",
      "Microsoft Translator\n",
      ", \n",
      "Google Translate\n",
      "), \n",
      "\n",
      "facial recognition\n",
      " (\n",
      "Apple\n",
      "'s \n",
      "Face ID\n",
      " or \n",
      "Microsoft\n",
      "'s \n",
      "DeepFace\n",
      ") and \n",
      "\n",
      "image labeling\n",
      " (used by \n",
      "Facebook\n",
      ", \n",
      "Apple\n",
      "'s \n",
      "iPhoto\n",
      " and \n",
      "TikTok\n",
      ").\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = requests.get('https://en.wikipedia.org/wiki/Artificial_intelligence')\n",
    "data = BeautifulSoup(response.text,'html.parser')\n",
    "app_sibling = data.find_all(title='Applications of artificial intelligence')[1].parent\n",
    "app = app_sibling.next_sibling.next_sibling.next_sibling.next_sibling\n",
    "for i in app.contents:\n",
    "    print(i.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09952c7",
   "metadata": {},
   "source": [
    "# Problem 5 : Image with maximum area\n",
    "From this website : https://en.wikipedia.org/wiki/Artificial_intelligence\n",
    "Find and print the src of the img tag which occupies the maximum area on the page.\n",
    "\n",
    "Note : Ignore images which doesn't have height or width attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1abef704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "//upload.wikimedia.org/wikipedia/commons/thumb/1/13/Joseph_Ayerle_portrait_of_Ornella_Muti_%28detail%29%2C_calculated_by_Artificial_Intelligence_%28AI%29_technology.jpg/220px-Joseph_Ayerle_portrait_of_Ornella_Muti_%28detail%29%2C_calculated_by_Artificial_Intelligence_%28AI%29_technology.jpg 56100\n"
     ]
    }
   ],
   "source": [
    "response = requests.get('https://en.wikipedia.org/wiki/Artificial_intelligence')\n",
    "data = BeautifulSoup(response.text,'html.parser')\n",
    "img_data = data.find_all('img')\n",
    "dic = {}\n",
    "for img in img_data:\n",
    "    img_data = str(img).split(\" \")\n",
    "    ht = 0\n",
    "    wt = 0\n",
    "    for i in img_data:\n",
    "        if 'height=' in i:\n",
    "            ht = int(re.search('\\d+',i).group())\n",
    "        if 'width=' in i:\n",
    "            wt = int(re.search('\\d+',i).group())\n",
    "    if ht!=0 and wt!=0:\n",
    "        dic[img['src']] = ht*wt\n",
    "\n",
    "keys = list(dic.keys())\n",
    "values = list(dic.values())\n",
    "max_value_index = np.argmax(values)\n",
    "print(keys[max_value_index],np.max(values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef29f8a",
   "metadata": {},
   "source": [
    "# Problem 6 : Quotes with tag humor\n",
    "Find all the quotes that have the tag as \"humor\" from this website https://quotes.toscrape.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "370cf7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”\n",
      "“A day without sunshine is like, you know, night.”\n",
      "“Anyone who thinks sitting in church can make you a Christian must also think that sitting in a garage can make you a car.”\n",
      "“Beauty is in the eye of the beholder and it may be necessary from time to time to give a stupid or misinformed beholder a black eye.”\n",
      "“All you need is love. But a little chocolate now and then doesn't hurt.”\n",
      "“Remember, we're madly in love, so it's all right to kiss me anytime you feel like it.”\n",
      "“Some people never go crazy. What truly horrible lives they must lead.”\n",
      "“The trouble with having an open mind, of course, is that people will insist on coming along and trying to put things in it.”\n",
      "“Think left and think right and think low and think high. Oh, the thinks you can think up if only you try!”\n",
      "“The reason I talk to myself is because I’m the only one whose answers I accept.”\n",
      "“I am free of all prejudice. I hate everyone equally. ”\n",
      "“A lady's imagination is very rapid; it jumps from admiration to love, from love to matrimony in a moment.”\n"
     ]
    }
   ],
   "source": [
    "url = 'https://quotes.toscrape.com'\n",
    "response = requests.get(url)\n",
    "data = BeautifulSoup(response.text,'html.parser')\n",
    "top_tags = data.find(class_='col-md-4 tags-box')\n",
    "tags = top_tags.find_all(class_='tag')\n",
    "for tag in tags:\n",
    "    if tag.string == 'humor':\n",
    "        humor_tag = tag['href']\n",
    "humor_url = url + humor_tag\n",
    "\n",
    "response = requests.get(humor_url)\n",
    "while response.status_code==200:\n",
    "    data = BeautifulSoup(response.text,'html.parser')\n",
    "    quotes = data.find_all(class_='text')\n",
    "    for quote in quotes:\n",
    "        print(quote.string)\n",
    "    next_url = data.find(class_='next')\n",
    "    if next_url is None:\n",
    "        break\n",
    "    humor_url = url + next_url.a['href']\n",
    "    response = requests.get(humor_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3130747",
   "metadata": {},
   "source": [
    "# Problem 7 : Print all authors\n",
    "Find and print the names of all the different authors from all pages of this https://quotes.toscrape.com/ website.\n",
    "\n",
    "Note : Print the names of all authors line wise sorted in dictionary order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cb6ba99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Albert Einstein',\n",
       " 'Alexandre Dumas fils',\n",
       " 'Alfred Tennyson',\n",
       " 'Allen Saunders',\n",
       " 'André Gide',\n",
       " 'Ayn Rand',\n",
       " 'Bob Marley',\n",
       " 'C.S. Lewis',\n",
       " 'Charles Bukowski',\n",
       " 'Charles M. Schulz',\n",
       " 'Douglas Adams',\n",
       " 'Dr. Seuss',\n",
       " 'E.E. Cummings',\n",
       " 'Eleanor Roosevelt',\n",
       " 'Elie Wiesel',\n",
       " 'Ernest Hemingway',\n",
       " 'Friedrich Nietzsche',\n",
       " 'Garrison Keillor',\n",
       " 'George Bernard Shaw',\n",
       " 'George Carlin',\n",
       " 'George Eliot',\n",
       " 'George R.R. Martin',\n",
       " 'Harper Lee',\n",
       " 'Haruki Murakami',\n",
       " 'Helen Keller',\n",
       " 'J.D. Salinger',\n",
       " 'J.K. Rowling',\n",
       " 'J.M. Barrie',\n",
       " 'J.R.R. Tolkien',\n",
       " 'James Baldwin',\n",
       " 'Jane Austen',\n",
       " 'Jim Henson',\n",
       " 'Jimi Hendrix',\n",
       " 'John Lennon',\n",
       " 'Jorge Luis Borges',\n",
       " 'Khaled Hosseini',\n",
       " \"Madeleine L'Engle\",\n",
       " 'Marilyn Monroe',\n",
       " 'Mark Twain',\n",
       " 'Martin Luther King Jr.',\n",
       " 'Mother Teresa',\n",
       " 'Pablo Neruda',\n",
       " 'Ralph Waldo Emerson',\n",
       " 'Stephenie Meyer',\n",
       " 'Steve Martin',\n",
       " 'Suzanne Collins',\n",
       " 'Terry Pratchett',\n",
       " 'Thomas A. Edison',\n",
       " 'W.C. Fields',\n",
       " 'William Nicholson'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://quotes.toscrape.com'\n",
    "response = requests.get(url)\n",
    "\n",
    "s = set()\n",
    "while response.status_code==200:\n",
    "    data = BeautifulSoup(response.text,'html.parser')\n",
    "    authors = data.find_all(class_='author')\n",
    "    for author in authors:\n",
    "        s.add(author.string)\n",
    "    next_url = data.find(class_='next')\n",
    "    if next_url is None:\n",
    "        break\n",
    "    response = requests.get(url + next_url.a['href'])\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f557b02",
   "metadata": {},
   "source": [
    "# Problem 8 : Birth Date of authors\n",
    "Find the birth date of authors whose name start with 'J' from this https://quotes.toscrape.com/ website.\n",
    "\n",
    "Note : Print a dictionary containing the name as key and the birth date as value.The Names of authors should be alphabetically sorted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f4b9fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'J.K. Rowling': 'July 31, 1965', 'Jane Austen': 'December 16, 1775', 'Jim Henson': 'September 24, 1936', 'Jorge Luis Borges': 'August 24, 1899', 'James Baldwin': 'August 02, 1924', 'J.R.R. Tolkien': 'January 03, 1892', 'J.D. Salinger': 'January 01, 1919', 'John Lennon': 'October 09, 1940', 'Jimi Hendrix': 'November 27, 1942', 'J.M. Barrie': 'May 09, 1860'}\n"
     ]
    }
   ],
   "source": [
    "url = 'https://quotes.toscrape.com'\n",
    "response = requests.get(url)\n",
    "\n",
    "dic = {}\n",
    "\n",
    "while response.status_code==200:\n",
    "    data = BeautifulSoup(response.text,'html.parser')\n",
    "    authors = data.find_all(class_='author')\n",
    "    for author in authors:\n",
    "        author_name = author.string\n",
    "        if author_name.startswith('J') and author_name not in dic:\n",
    "            about_url = url + author.parent.a['href']\n",
    "            resp = requests.get(about_url)\n",
    "            author_data = BeautifulSoup(resp.text,'html.parser')\n",
    "            birthDate = author_data.find(class_='author-born-date').string\n",
    "            dic[author_name] = birthDate\n",
    "    next_url = data.find(class_='next')\n",
    "    if next_url is None:\n",
    "        break\n",
    "    response = requests.get(url + next_url.a['href'])\n",
    "print(dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1912e3",
   "metadata": {},
   "source": [
    "# Problem 9 : Quotes by Albert Einstein\n",
    "Find all the quotes by Albert Einstein(in the order they appear on the page) from this website http://quotes.toscrape.com/\n",
    "\n",
    "Note : Fetch data from all the pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7fffd02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”\n",
      "“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”\n",
      "“Try not to become a man of success. Rather become a man of value.”\n",
      "“If you can't explain it to a six year old, you don't understand it yourself.”\n",
      "“If you want your children to be intelligent, read them fairy tales. If you want them to be more intelligent, read them more fairy tales.”\n",
      "“Logic will get you from A to Z; imagination will get you everywhere.”\n",
      "“Any fool can know. The point is to understand.”\n",
      "“Life is like riding a bicycle. To keep your balance, you must keep moving.”\n",
      "“If I were not a physicist, I would probably be a musician. I often think in music. I live my daydreams in music. I see my life in terms of music.”\n",
      "“Anyone who has never made a mistake has never tried anything new.”\n"
     ]
    }
   ],
   "source": [
    "url = 'http://quotes.toscrape.com'\n",
    "response = requests.get(url)\n",
    "\n",
    "while response.status_code==200:\n",
    "    data = BeautifulSoup(response.text,'html.parser')\n",
    "    authors = data.find_all(class_='author')\n",
    "    for author in authors:\n",
    "        if author.string == 'Albert Einstein':\n",
    "            quote = author.parent.parent.find(class_='text')\n",
    "            print(quote.string)\n",
    "    next_url = data.find(class_='next')\n",
    "    if next_url is None:\n",
    "        break\n",
    "    response = requests.get(url + next_url.a['href'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
